{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "1. Introduction -- what is web scraping?\n",
    "2. Getting started\n",
    "    - Libraries\n",
    "    - Web technology background\n",
    "3. HTML and CSS\n",
    "4. BeautifulSoup\n",
    "    - Retrieving documents and parsing them\n",
    "    - Using CSS to retrieve pieces of documents\n",
    "5. Scrapy\n",
    "    - Building a simple spider\n",
    "    - Buliding a more complex spider project\n",
    "6. Scrapy settings and debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The original idea of the Web was that there would be documents, and they would be marked up with HTML (a tagging system). Over time, several things happened:\n",
    "\n",
    "- HTML pages became dynamically produced. They no longer had to represent an actual document on a disk somewhere.\n",
    "- CSS (cascading stylesheets) are a separate technology and language in the browser, alongside the HTML, that describes how things should look (and to some degree, how they should behave)\n",
    "- JavaScript is also (usually, somehow) inside of the Web page, and it provides computation that runs inside of your browser, interacting with the HTML and CSS, and also the user's mouse clicks, keyboard entry, etc.\n",
    "\n",
    "As more and more information was put on to the Web, we wanted to be able to find and extract it using software. The idea of \"crawling the Web\" or \"scraping the Web\" became a big thing. \n",
    "\n",
    "If you want to scrape a Web page, it doesn't sound like it should be so hard. And there are libraries that you can use to parse the HTML. But those are kind of brittle and annoying, plus you want something at a higher level -- either to deal with HTML pages at a higher level, or even the whole process of searching + scraping at a higher level.\n",
    "\n",
    "Before you scrape a Web site, you should be sure that you have permission to store + use the content you get from there.\n",
    "\n",
    "Another issue: Web scraping can really affect the performance of a Web server. There are standard describing how much you can retrieve from a site, and what you're allowed to view. This is especially put in a file called `robots.txt`. That file indicates what can and cannot be retrieved automatically.\n",
    "\n",
    "Your browser is an HTTP client; it sends a request to the HTTP server. That request basically says, \"Give me document xyz.\" The simplest possible request is what we call `GET`. Along with that request, we'll send a bunch of HTTP request headers, basically a dict indicating what sort of response we want, plus metadata might want to use.\n",
    "\n",
    "The server then returns a *response* to us. The response will have a numeric code (200 == OK, 404 == no such file, etc.) The response will also have content. That content can be in HTML.\n",
    "\n",
    "When we make that request to the server, we send (among other things) a User-Agent header, indicating what kind of browser we're using.\n",
    "\n",
    "It's very common for programmers to think that is a problem (scraping HTML) that we can solve with regular expressions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we scrape the Web?\n",
    "\n",
    "- Data inside of HTML pages\n",
    "- Text inside of HTML pages\n",
    "- Cataloging of content\n",
    "- Monitoring and/or retrieving data from our competitors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are we going to use?\n",
    "\n",
    "- `requests` -- an HTTP client library in Python\n",
    "- `BeautifulSoup` -- a parser for HTML pages that works on data we've already downloaded\n",
    "- `Scrapy` -- all-in-one toolkit for creating spiders that retrieve from multiple sites/pages, and then let us extract and process that data in a number of different ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's talk about HTTP\n",
    "\n",
    "When we make a request to a server, we're most commonly using a `GET` request.\n",
    "\n",
    "    GET /myfile.txt HTTP/1.0\n",
    "\n",
    "There are other verbs, as well:\n",
    "\n",
    "`POST` is the most common, by far. \n",
    "\n",
    "Why do we have these verbs?\n",
    "\n",
    "Conventionally, `GET` is used when we want to retrieve a file/resource, and maybe we want to pass a few name-value pairs along with the request, but not too much. Those can go in the URL.\n",
    "\n",
    "    https://mysite.com?x=10&y=20\n",
    "\n",
    "`POST` is meant, at least in theory, for when we're submitting data. If you fill out a form, then it's typically submitted using `POST`. The data that can be sent is much larger and more structured than what can be done with a `GET` request.  There are some other verbs as well, and some sites implement them and do things with them, but not that many.\n",
    "\n",
    "When we send our request, we'll include a bunch of request headers.\n",
    "\n",
    "When we get our response, we'll get a status code (number) plus a bunch of response headers plus the content (we can hope)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `requests`\n",
    "\n",
    "The `requests` library makes it easy to do this sort thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('https://python.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection....................: keep-alive\n",
      "Content-Length................: 50629\n",
      "Content-Type..................: text/html; charset=utf-8\n",
      "X-Frame-Options...............: SAMEORIGIN\n",
      "Via...........................: 1.1 varnish, 1.1 varnish\n",
      "Accept-Ranges.................: bytes\n",
      "Date..........................: Mon, 26 Aug 2024 15:31:20 GMT\n",
      "Age...........................: 1485\n",
      "X-Served-By...................: cache-iad-kiad7000025-IAD, cache-fra-etou8220046-FRA\n",
      "X-Cache.......................: HIT, HIT\n",
      "X-Cache-Hits..................: 7, 4\n",
      "X-Timer.......................: S1724686280.469753,VS0,VE0\n",
      "Vary..........................: Cookie\n",
      "Strict-Transport-Security.....: max-age=63072000; includeSubDomains; preload\n"
     ]
    }
   ],
   "source": [
    "for key, value in r.headers.items():\n",
    "    print(f'{key:.<30}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
